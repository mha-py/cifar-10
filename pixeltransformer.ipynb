{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02f72e-5c2b-4a00-91cc-39698c33af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixeltransformer, Ã¤hnlich wie bei pixelcnn und pixelrnn geht es darum,\n",
    "# die pixelwerte vorherzusagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5980338-e503-4423-94c4-3dd626c339ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pickle as pkl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *\n",
    "from layers import *\n",
    "from transformer import *\n",
    "from cifar10 import *\n",
    "\n",
    "SX = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8f637-db7f-449c-b5dd-ce76201deb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "showimg(getimg(1))\n",
    "getlabel(1)\n",
    "\n",
    "showimg(randomcrop(getimg(1), s=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff962461-e649-4b32-8330-3254f8ca2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#counter = Counter(list((randomcrop(getimg(4), s=8)*255).astype(int).flatten()))\n",
    "#sum([ -np.log(n/192)*n/192 for p, n in counter.items() ]) # <-- Anzahl an bits pro pixelkanal ist ca. 4.5 bits/kanal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865447ff-8053-4275-8fa8-07431bc7d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batchgen(bsize=16, start=500):\n",
    "    ep = 0\n",
    "    while True:\n",
    "        inds = np.random.permutation(range(start, N))\n",
    "        minibatches = [ inds[k*bsize:(k+1)*bsize] for k in range(len(inds)//bsize) ]\n",
    "        for mb in minibatches:\n",
    "            xs = []\n",
    "            ys = []\n",
    "            for i in mb:\n",
    "                x = randomcrop(getimg(i), s=8)\n",
    "                x = list(x.flatten())\n",
    "                x = [257/256] + x # start \"token\"\n",
    "                xs.append(x)\n",
    "            xs = np.array(xs)\n",
    "            xs = (255*xs).astype(int)\n",
    "            ys = xs[:,1:]\n",
    "            xs = xs[:,:-1]\n",
    "            \n",
    "            yield xs, ys\n",
    "        print(f'========== EPOCH {ep} COMPLETED ==========')\n",
    "        ep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da198c9-671d-4433-97b3-8cd536ad64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = batchgen()\n",
    "xs, ys = next(bg)\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28ccf1-79f8-44f2-90a9-15560136485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from layers import *\n",
    "\n",
    "\n",
    "def _Att(q, k, v, mask=None, bias=None):\n",
    "    b, h, i, m = q.shape\n",
    "    b, h, j, m = k.shape\n",
    "    b, h, j, n = v.shape\n",
    "    \n",
    "    beta = torch.einsum('bhim, bhjm -> bhij', q, k) / np.sqrt(m)\n",
    "    if bias is not None:\n",
    "        beta = beta + bias\n",
    "\n",
    "    if mask is not None:\n",
    "        beta = beta.masked_fill(mask == 0, -1e12)\n",
    "\n",
    "    beta = F.softmax(beta, dim=-1)\n",
    "    \n",
    "    if mask is not None:\n",
    "        beta = beta.masked_fill(mask == 0, 0)  # make sure its really closed\n",
    "\n",
    "    o = torch.einsum('bhij,bhjn->bhin', beta, v)\n",
    "    \n",
    "    return o, beta\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n, m, nh, p):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(n, m)\n",
    "        self.k = nn.Linear(n, m)\n",
    "        self.v = nn.Linear(n, n)\n",
    "        self.p = nn.Linear(n, n)\n",
    "        self.bias = nn.Parameter(torch.zeros(1, nh, p, p))\n",
    "        self.nh = nh\n",
    "\n",
    "    def forward(self, x, y, z, mask=None):\n",
    "\n",
    "        q = rearrange(self.q(x), 'b p (h n) -> b h p n', h=self.nh)\n",
    "        k = rearrange(self.k(y), 'b p (h n) -> b h p n', h=self.nh)\n",
    "        v = rearrange(self.v(z), 'b p (h n) -> b h p n', h=self.nh)\n",
    "\n",
    "        x, self.beta = _Att(q, k, v, mask, self.bias.repeat(len(x), 1, 1, 1))\n",
    "        x = rearrange(x, 'b h p n -> b p (h n)', h=self.nh)\n",
    "        \n",
    "        x = self.p(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n, m, nh, p):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n, m, nh, p)\n",
    "        self.ln = LayerNorm(n)\n",
    "        self.ff = FeedForwardLayer(n)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, mask=None):\n",
    "        y = self.ln(x)\n",
    "        x = x + self.dropout(self.mha(y, y, y, mask))\n",
    "        x = self.ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n, nh, p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(256+10, n)\n",
    "        \n",
    "        self.posenc = PositionalEncoding(n)\n",
    "\n",
    "        self.ln1 = LayerNorm(n)\n",
    "        self.enc1 = EncoderBlock(n, n, nh, p)\n",
    "        self.enc2 = EncoderBlock(n, n, nh, p)\n",
    "        self.enc3 = EncoderBlock(n, n, nh, p)\n",
    "        self.enc4 = EncoderBlock(n, n, nh, p)\n",
    "        self.enc5 = EncoderBlock(n, n, nh, p)\n",
    "        self.enc6 = EncoderBlock(n, n, nh, p)\n",
    "        self.ln2 = LayerNorm(n)\n",
    "\n",
    "        self.dense = nn.Linear(n, 256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.n = n\n",
    "        self.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mask = np2t(np.tri(x.shape[1])[None]).cuda()\n",
    "        \n",
    "        #x = rearrange(x, 'b (p n) -> b p n', n=1)\n",
    "        #x = gelu(self.predense(x))\n",
    "        x = self.emb(x)\n",
    "        x = self.posenc(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.ln1(x)\n",
    "        x = self.enc1(x, mask)\n",
    "        x = self.enc2(x, mask)\n",
    "        x = self.enc3(x, mask)\n",
    "        x = self.enc4(x, mask)\n",
    "        x = self.ln2(x)\n",
    "        \n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b58ae5-e076-486b-bd53-62fc6b2cfa93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Net(256, 4, p=8*8*3)\n",
    "from torch_optimizer import Lookahead, Yogi\n",
    "net.optim = Lookahead(Yogi(net.parameters(), lr=3e-3))\n",
    "net.iters = 0\n",
    "net.losses = []\n",
    "net.vlosses = []\n",
    "net.vaccs = []\n",
    "bg = batchgen()\n",
    "\n",
    "from torchsummary import summary\n",
    "#summary(net, (8*8*3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47de9be-ee5f-4404-8453-a926c8e78aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valloss():\n",
    "    net.eval()\n",
    "    xs = []\n",
    "    for i in range(100):\n",
    "        x = randomcrop(getimg(i), s=8)\n",
    "        x = list(x.flatten())\n",
    "        x = [257/256] + x # start \"token\"\n",
    "        xs.append(x)\n",
    "    xs = np.array(xs)\n",
    "    xs = (255*xs).astype(int)\n",
    "    ys = xs[:,1:]\n",
    "    xs = xs[:,:-1]\n",
    "    xs, ys = np2t(xs, ys)\n",
    "    yp = net(xs.long())\n",
    "\n",
    "    yp = yp[:,-12:]\n",
    "    ys = ys[:,-12:]\n",
    "    \n",
    "    yp = yp.reshape(-1, 256)\n",
    "    ys = ys.reshape(-1).long()\n",
    "    return F.nll_loss(F.log_softmax(yp, dim=1), ys) / np.log(2)\n",
    "    \n",
    "def loss():\n",
    "    net.train()\n",
    "    xs, ys = next(bg)\n",
    "    xs, ys = np2t(xs, ys)\n",
    "    yp = net(xs.long())\n",
    "    yp = yp.reshape(-1, 256)\n",
    "    ys = ys.reshape(-1).long()\n",
    "    return F.nll_loss(F.log_softmax(yp, dim=1), ys) / np.log(2)\n",
    "\n",
    "valloss()\n",
    "loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb80bcf-ae61-4224-b4cc-dd003d1145eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for k in trange(999999):\n",
    "    net.train()\n",
    "    l = loss()\n",
    "    l.backward()\n",
    "    losses.append(l.item())\n",
    "    net.optim.step()\n",
    "    net.zero_grad()\n",
    "\n",
    "    if len(losses) == 50:\n",
    "        net.vlosses.append((net.iters, valloss().item()))\n",
    "        net.losses.append((net.iters, np.mean(losses)))\n",
    "        losses = []\n",
    "\n",
    "    if k % 50 == 0:\n",
    "        plt.plot(*zip(*net.losses))\n",
    "        plt.plot(*zip(*net.vlosses))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    net.iters += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
